{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e96c4e7-5353-4425-9b05-8a414c7f7a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name Stars                                              Title  \\\n",
      "0   Aidan Shane Wade   5.0                     5.0 out of 5 stars\\nVery fast!   \n",
      "1       Rich and Jen   5.0    5.0 out of 5 stars\\nShe's a beast, but so HOT!!   \n",
      "2       Amazon lover   5.0  5.0 out of 5 stars\\nRunning air cooled on Deep...   \n",
      "3            Eric E.   5.0  5.0 out of 5 stars\\nMuch faster than my previo...   \n",
      "4              Rdawg   5.0              5.0 out of 5 stars\\nPowerful but Warm   \n",
      "..               ...   ...                                                ...   \n",
      "95           Chris H   5.0  5.0 out of 5 stars\\ni7-14700K is a significant...   \n",
      "96                AK   4.0   4.0 out of 5 stars\\nInsane performance at a cost   \n",
      "97                 J   4.0  4.0 out of 5 stars\\nFast, but unstable when ov...   \n",
      "98             Deven   5.0  5.0 out of 5 stars\\n6.2GHZ 8200mts cl38 Daily ...   \n",
      "99   JZ the reviewer   5.0  5.0 out of 5 stars\\nIF YOU WANT THE AIR COOLED...   \n",
      "\n",
      "          Date                                        Description  \n",
      "0   05/07/2024  It’s very capable for modern gaming without di...  \n",
      "1   03/01/2024  I guess everyone is right, these things are ho...  \n",
      "2   03/11/2023  So far so good.  This CPU is a monster and run...  \n",
      "3   19/03/2024  So I finally upgraded my home workstation, and...  \n",
      "4   08/05/2024  Before you purchase this CPU please also purch...  \n",
      "..         ...                                                ...  \n",
      "95  31/05/2024  I decided the difference between the i7 and th...  \n",
      "96  02/06/2024  This processor is a speed demon. Gaming, editi...  \n",
      "97  11/01/2024  I built 3 systems using 14900 series. They all...  \n",
      "98  04/04/2024  This professional CPU isn't designed for the a...  \n",
      "99  11/02/2024  I'm not going to go over the same performance ...  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/i9-14900K-Desktop-Processor-Integrated-Graphics/product-reviews/B0CGJDKLB8/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "\n",
    "# Define Page No\n",
    "len_page = 10\n",
    "\n",
    "### <font color=\"red\">Functions</font>\n",
    "\n",
    "# Extra Data as Html object from amazon Review page\n",
    "def reviewsHtml(url, len_page):\n",
    "    \n",
    "    # Empty List define to store all pages html data\n",
    "    soups = []\n",
    "    \n",
    "    # Loop for gather all 3000 reviews from 300 pages via range\n",
    "    for page_no in range(1, len_page + 1):\n",
    "        \n",
    "        # parameter set as page no to the requests body\n",
    "        params = {\n",
    "            'ie': 'UTF8',\n",
    "            'reviewerType': 'all_reviews',\n",
    "            'filterByStar': 'critical',\n",
    "            'pageNumber': page_no,\n",
    "        }\n",
    "        \n",
    "        # Request make for each page\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Save Html object by using BeautifulSoup4 and lxml parser\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Add single Html page data in master soups list\n",
    "        soups.append(soup)\n",
    "        \n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "### <font color=\"red\">Data Process</font>\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url, len_page)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d2dc38-69dc-49af-96f5-802a27c44d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name Stars                                              Title  \\\n",
      "0  Aidan Shane Wade   5.0                     5.0 out of 5 stars\\nVery fast!   \n",
      "1      Rich and Jen   5.0    5.0 out of 5 stars\\nShe's a beast, but so HOT!!   \n",
      "2      Amazon lover   5.0  5.0 out of 5 stars\\nRunning air cooled on Deep...   \n",
      "3           Eric E.   5.0  5.0 out of 5 stars\\nMuch faster than my previo...   \n",
      "4             Rdawg   5.0              5.0 out of 5 stars\\nPowerful but Warm   \n",
      "5           Chris H   5.0  5.0 out of 5 stars\\ni7-14700K is a significant...   \n",
      "6                AK   4.0   4.0 out of 5 stars\\nInsane performance at a cost   \n",
      "7                 J   4.0  4.0 out of 5 stars\\nFast, but unstable when ov...   \n",
      "8             Deven   5.0  5.0 out of 5 stars\\n6.2GHZ 8200mts cl38 Daily ...   \n",
      "9   JZ the reviewer   5.0  5.0 out of 5 stars\\nIF YOU WANT THE AIR COOLED...   \n",
      "\n",
      "         Date                                        Description  \n",
      "0  05/07/2024  It’s very capable for modern gaming without di...  \n",
      "1  03/01/2024  I guess everyone is right, these things are ho...  \n",
      "2  03/11/2023  So far so good.  This CPU is a monster and run...  \n",
      "3  19/03/2024  So I finally upgraded my home workstation, and...  \n",
      "4  08/05/2024  Before you purchase this CPU please also purch...  \n",
      "5  31/05/2024  I decided the difference between the i7 and th...  \n",
      "6  02/06/2024  This processor is a speed demon. Gaming, editi...  \n",
      "7  11/01/2024  I built 3 systems using 14900 series. They all...  \n",
      "8  04/04/2024  This professional CPU isn't designed for the a...  \n",
      "9  11/02/2024  I'm not going to go over the same performance ...  \n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/i9-14900K-Desktop-Processor-Integrated-Graphics/product-reviews/B0CGJDKLB8/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "\n",
    "# Define Page No\n",
    "len_page = 1\n",
    "\n",
    "### <font color=\"red\">Functions</font>\n",
    "\n",
    "# Extra Data as Html object from amazon Review page\n",
    "def reviewsHtml(url, len_page):\n",
    "    \n",
    "    # Empty List define to store all pages html data\n",
    "    soups = []\n",
    "    \n",
    "    # Loop for gather all 3000 reviews from 300 pages via range\n",
    "    for page_no in range(1, len_page + 1):\n",
    "        \n",
    "        # parameter set as page no to the requests body\n",
    "        params = {\n",
    "            'ie': 'UTF8',\n",
    "            'reviewerType': 'all_reviews',\n",
    "            'filterByStar': 'critical',\n",
    "            'pageNumber': page_no,\n",
    "        }\n",
    "        \n",
    "        # Request make for each page\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Save Html object by using BeautifulSoup4 and lxml parser\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Add single Html page data in master soups list\n",
    "        soups.append(soup)\n",
    "        \n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "### <font color=\"red\">Data Process</font>\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url, len_page)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a0ea9f-b01a-4095-93fa-9b0a6b2fa719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name Stars  \\\n",
      "0                                 An Amazon Customer   5.0   \n",
      "1  Cpu was not in the box as it was advertised. C...   3.0   \n",
      "2                              I never received it .   5.0   \n",
      "3                                Jesse Ryan Peterson   5.0   \n",
      "4                                       Bryson Lewis   4.0   \n",
      "5                                            Mikeiza   4.0   \n",
      "6                                              Shawn   5.0   \n",
      "7                                       A. L. Fowler   5.0   \n",
      "8                                            Michael   4.0   \n",
      "9                                          Nice Pony   5.0   \n",
      "\n",
      "                                               Title        Date  \\\n",
      "0  5.0 out of 5 stars\\nVery happy with 14900K, th...  04/11/2023   \n",
      "1     3.0 out of 5 stars\\nRegret, should have go AMD  03/05/2024   \n",
      "2        5.0 out of 5 stars\\nPowerful cpu for gaming  20/02/2024   \n",
      "3                     5.0 out of 5 stars\\nGaming God  04/05/2024   \n",
      "4           4.0 out of 5 stars\\nGets Hot Under Loads  07/01/2024   \n",
      "5       4.0 out of 5 stars\\nStable, fast and great!!  17/02/2024   \n",
      "6          5.0 out of 5 stars\\nWorth it if <13th gen  17/11/2023   \n",
      "7  5.0 out of 5 stars\\nIntel has made a really fa...  26/01/2024   \n",
      "8    4.0 out of 5 stars\\nDecent upgrade over I7 8700  16/01/2024   \n",
      "9         5.0 out of 5 stars\\nIntel® Core™ i7-14700K  28/03/2024   \n",
      "\n",
      "                                         Description  \n",
      "0  This CPU is amazing. Feels like magic. I have ...  \n",
      "1  Because of the crash problem and we are told t...  \n",
      "2  Love the processor, got it new and using with ...  \n",
      "3  The media could not be loaded.\\n              ...  \n",
      "4  Undervolt for best performance. Don’t get me w...  \n",
      "5           Greaaaaaat, but lacking in presentation.  \n",
      "6  Was going to wait until 15th gen to upgrade my...  \n",
      "7  I like the CPU because of the speed and easy i...  \n",
      "8  Upgrade from an I7 8700 to this. Temps are cra...  \n",
      "9  This new Intel® Core™ i7-14700K New Gaming Des...  \n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/i9-14900K-Desktop-Processor-Integrated-Graphics/product-reviews/B0CGJDKLB8/ref=cm_cr_getr_d_paging_btm_next_5?ie=UTF8&reviewerType=all_reviews&pageNumber=10'\n",
    "\n",
    "# Define Page No\n",
    "len_page = 1\n",
    "\n",
    "### <font color=\"red\">Functions</font>\n",
    "\n",
    "# Extra Data as Html object from amazon Review page\n",
    "def reviewsHtml(url, len_page):\n",
    "    \n",
    "    # Empty List define to store all pages html data\n",
    "    soups = []\n",
    "    \n",
    "    # Loop for gather all 3000 reviews from 300 pages via range\n",
    "    for page_no in range(1, len_page + 1):\n",
    "        \n",
    "        # parameter set as page no to the requests body\n",
    "        params = {\n",
    "            'ie': 'UTF8',\n",
    "            'reviewerType': 'all_reviews',\n",
    "            'filterByStar': 'critical',\n",
    "            'pageNumber': page_no,\n",
    "        }\n",
    "        \n",
    "        # Request make for each page\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Save Html object by using BeautifulSoup4 and lxml parser\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Add single Html page data in master soups list\n",
    "        soups.append(soup)\n",
    "        \n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "### <font color=\"red\">Data Process</font>\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url, len_page)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74971b8-06d2-454d-99cc-4998357afea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '←Previous page'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_dicts\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Grab all HTML\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m html_datas \u001b[38;5;241m=\u001b[39m \u001b[43mreviewsHtml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Empty List to Hold all reviews data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m reviews \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m, in \u001b[0;36mreviewsHtml\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Find the total number of pages\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     total_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_one\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.a-pagination .a-disabled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     total_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '←Previous page'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/Intel-i9-13900K-Desktop-Processor-P-cores/product-reviews/B0BCF54SR1/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1'\n",
    "\n",
    "def reviewsHtml(url):\n",
    "    # Empty list to store all pages html data\n",
    "    soups = []\n",
    "\n",
    "    # Initial request to get the total number of pages\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Find the total number of pages\n",
    "    try:\n",
    "        total_pages = int(soup.select_one('.a-pagination .a-disabled').text.strip())\n",
    "    except AttributeError:\n",
    "        total_pages = 1\n",
    "\n",
    "    # Loop to gather all reviews from all pages\n",
    "    for page_no in range(1, total_pages + 1):\n",
    "        print(f'Fetching page {page_no} of {total_pages}')\n",
    "        response = requests.get(url.replace('pageNumber=1', f'pageNumber={page_no}'), headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soups.append(soup)\n",
    "\n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6ce193-cc8f-4722-a2da-a6ab73cc095f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /Intel-i5-13600KF-Desktop-Processor-P-cores/product-reviews/B0BCF5CZ16/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D2A47F63F0>: Failed to resolve 'www.amazon.com' ([Errno 11002] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:964\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    963\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    965\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11002] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002D2A47F63F0>: Failed to resolve 'www.amazon.com' ([Errno 11002] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /Intel-i5-13600KF-Desktop-Processor-P-cores/product-reviews/B0BCF5CZ16/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D2A47F63F0>: Failed to resolve 'www.amazon.com' ([Errno 11002] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_dicts\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Grab all HTML\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m html_datas \u001b[38;5;241m=\u001b[39m \u001b[43mreviewsHtml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Empty List to Hold all reviews data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m reviews \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mreviewsHtml\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     20\u001b[0m soups \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Initial request to get the total number of pages\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Find the total number of pages\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /Intel-i5-13600KF-Desktop-Processor-P-cores/product-reviews/B0BCF5CZ16/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D2A47F63F0>: Failed to resolve 'www.amazon.com' ([Errno 11002] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/Intel-i5-13600KF-Desktop-Processor-P-cores/product-reviews/B0BCF5CZ16/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1'\n",
    "\n",
    "def reviewsHtml(url):\n",
    "    # Empty list to store all pages html data\n",
    "    soups = []\n",
    "\n",
    "    # Initial request to get the total number of pages\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Find the total number of pages\n",
    "    try:\n",
    "        total_pages = int(soup.select('.a-pagination li')[-2].text.strip())\n",
    "    except (AttributeError, ValueError, IndexError):\n",
    "        total_pages = 25\n",
    "\n",
    "    # Loop to gather all reviews from all pages\n",
    "    for page_no in range(1, total_pages + 1):\n",
    "        print(f'Fetching page {page_no} of {total_pages}')\n",
    "        response = requests.get(url.replace('pageNumber=1', f'pageNumber={page_no}'), headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soups.append(soup)\n",
    "\n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "828f7cbb-3049-43b7-96dc-d5ca1a6d50b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNS resolution successful: 162.219.225.118\n",
      "HTTP request successful: 503\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import requests\n",
    "\n",
    "# Check DNS resolution\n",
    "try:\n",
    "    ip = socket.gethostbyname('www.amazon.com')\n",
    "    print(f'DNS resolution successful: {ip}')\n",
    "except socket.gaierror as e:\n",
    "    print(f'DNS resolution failed: {e}')\n",
    "\n",
    "# Check HTTP connectivity\n",
    "try:\n",
    "    response = requests.get('https://www.amazon.com', timeout=10)\n",
    "    print(f'HTTP request successful: {response.status_code}')\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'HTTP request failed: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baabdecc-ba0a-40b6-a683-3240ccf4a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 of 10\n",
      "Fetching page 2 of 10\n",
      "Fetching page 3 of 10\n",
      "Fetching page 4 of 10\n",
      "Fetching page 5 of 10\n",
      "Fetching page 6 of 10\n",
      "Fetching page 7 of 10\n",
      "Fetching page 8 of 10\n",
      "Fetching page 9 of 10\n",
      "Fetching page 10 of 10\n",
      "               Name Stars                                              Title  \\\n",
      "0            YALE70   5.0  5.0 out of 5 stars\\nThe performance bargain of...   \n",
      "1     Evan Herriges   5.0        5.0 out of 5 stars\\nAmazingly underated cpu   \n",
      "2          Vladimir   5.0           5.0 out of 5 stars\\nWhat an amazing CPU!   \n",
      "3   Amazon Customer   5.0          5.0 out of 5 stars\\nComing from i5 12400f   \n",
      "4       Dustin Reed   5.0  5.0 out of 5 stars\\nWay better deal then Best ...   \n",
      "..              ...   ...                                                ...   \n",
      "95    Bruno Andrade   N/A                          O sweetspot da geração 13   \n",
      "96             tuna   N/A                                 Too good for an i5   \n",
      "97       ProjectMan   N/A                                       Solid i5 CPU   \n",
      "98   Cliente Kindle   N/A                                          Topissimo   \n",
      "99             Luis   N/A                                  Mucho rendimiento   \n",
      "\n",
      "          Date                                        Description  \n",
      "0   22/02/2023  I won't go into much technical detail here sin...  \n",
      "1   01/06/2024  Been using this cpu for about a year now and i...  \n",
      "2   28/03/2024  Using it with arctic freezer 34, even overcloc...  \n",
      "3   11/04/2024  I was on the fence about upgrading from i5 124...  \n",
      "4   09/07/2024  Should’ve went this route this cpu performs re...  \n",
      "..         ...                                                ...  \n",
      "95  11/05/2023  Cpu bruto, custo benefício pelo que entrega, n...  \n",
      "96  28/07/2023  While I've been trying to build a PC as a blen...  \n",
      "97  16/04/2024  13th gen Intel CPU is the price sweet point ri...  \n",
      "98  30/01/2024  O processador é ótimo coloquei um water cooler...  \n",
      "99  17/11/2023  Sirve al 100 para gaming con cualquier tarjeta...  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/Intel-i5-13600KF-Desktop-Processor-P-cores/product-reviews/B0BCF5CZ16/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1'\n",
    "\n",
    "def reviewsHtml(url):\n",
    "    # Empty list to store all pages html data\n",
    "    soups = []\n",
    "\n",
    "    # Initial request to get the total number of pages\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Find the total number of pages\n",
    "    try:\n",
    "        total_pages = int(soup.select('.a-pagination li')[-2].text.strip())\n",
    "    except (AttributeError, ValueError, IndexError):\n",
    "        total_pages = 10\n",
    "\n",
    "    # Loop to gather all reviews from all pages\n",
    "    for page_no in range(1, total_pages + 1):\n",
    "        print(f'Fetching page {page_no} of {total_pages}')\n",
    "        response = requests.get(url.replace('pageNumber=1', f'pageNumber={page_no}'), headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soups.append(soup)\n",
    "\n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a13bbd4-2098-4e4a-a746-228b340357b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 of 10\n",
      "Fetching page 2 of 10\n",
      "Fetching page 3 of 10\n",
      "Fetching page 4 of 10\n",
      "Fetching page 5 of 10\n",
      "Fetching page 6 of 10\n",
      "Fetching page 7 of 10\n",
      "Fetching page 8 of 10\n",
      "Fetching page 9 of 10\n",
      "Fetching page 10 of 10\n",
      "                                 Name Stars  \\\n",
      "0                                rags   5.0   \n",
      "1                            Thomas G   5.0   \n",
      "2                Robert L. Schlessman   5.0   \n",
      "3                              billie   5.0   \n",
      "4                        Amazon buyer   5.0   \n",
      "..                                ...   ...   \n",
      "95                      Consumer Bias   5.0   \n",
      "96                      Curtis Lysher   5.0   \n",
      "97  Carlos Alberto Hernández Mosquera   5.0   \n",
      "98                             Joseph   5.0   \n",
      "99                            Anthony   N/A   \n",
      "\n",
      "                                                Title        Date  \\\n",
      "0   5.0 out of 5 stars\\nPrice to performance is great  25/07/2022   \n",
      "1   5.0 out of 5 stars\\nExtremely good CPU, albeit...  07/04/2021   \n",
      "2            5.0 out of 5 stars\\nExcellent processor!  24/05/2022   \n",
      "3   5.0 out of 5 stars\\nShouldn't have bought it, ...  16/03/2023   \n",
      "4                     5.0 out of 5 stars\\nWorks great  13/02/2024   \n",
      "..                                                ...         ...   \n",
      "95                5.0 out of 5 stars\\n4.8Ghz all core  07/07/2022   \n",
      "96  5.0 out of 5 stars\\nFast processor at the time...  01/06/2023   \n",
      "97            5.0 out of 5 stars\\nMuy buen procesador  27/05/2024   \n",
      "98  5.0 out of 5 stars\\nEasy to install, and works...  12/07/2023   \n",
      "99  Product us good but they took 20+ extra days t...  12/10/2022   \n",
      "\n",
      "                                          Description  \n",
      "0   The i5 11600k is, at the time of this writing,...  \n",
      "1   Using this in the ASUS TUF Gaming Z590-Plus Wi...  \n",
      "2   Excellent mid-range processor with a reasonabl...  \n",
      "3   I upgraded from a 10600kf to the 11600k. Its a...  \n",
      "4   I'm not a computer person but I know one and h...  \n",
      "..                                                ...  \n",
      "95  Easy to overclock with Intel XTU software. I s...  \n",
      "96  Good enough for gaming, basic CAD and 3d desig...  \n",
      "97            Excelente desempeño en múltiples tareas  \n",
      "98  I had high expectations of this processor beca...  \n",
      "99                                             Sjsjsj  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Header to set the requests as a browser requests\n",
    "headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# URL of The amazon Review page\n",
    "reviews_url = 'https://www.amazon.com/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1'\n",
    "\n",
    "def reviewsHtml(url):\n",
    "    # Empty list to store all pages html data\n",
    "    soups = []\n",
    "\n",
    "    # Initial request to get the total number of pages\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Find the total number of pages\n",
    "    try:\n",
    "        total_pages = int(soup.select('.a-pagination li')[-2].text.strip())\n",
    "    except (AttributeError, ValueError, IndexError):\n",
    "        total_pages = 10\n",
    "\n",
    "    # Loop to gather all reviews from all pages\n",
    "    for page_no in range(1, total_pages + 1):\n",
    "        print(f'Fetching page {page_no} of {total_pages}')\n",
    "        response = requests.get(url.replace('pageNumber=1', f'pageNumber={page_no}'), headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soups.append(soup)\n",
    "\n",
    "    return soups\n",
    "\n",
    "# Grab Reviews name, description, date, stars, title from HTML\n",
    "def getReviews(html_data):\n",
    "\n",
    "    # Create Empty list to Hold all data\n",
    "    data_dicts = []\n",
    "    \n",
    "    # Select all Reviews BOX html using css selector\n",
    "    boxes = html_data.select('div[data-hook=\"review\"]')\n",
    "    \n",
    "    # Iterate all Reviews BOX \n",
    "    for box in boxes:\n",
    "        \n",
    "        # Select Name using css selector and cleaning text using strip()\n",
    "        # If Value is empty define value with 'N/A' for all.\n",
    "        try:\n",
    "            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "        except Exception as e:\n",
    "            stars = 'N/A'   \n",
    "\n",
    "        try:\n",
    "            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Convert date str to dd/mm/yyy format\n",
    "            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
    "        except Exception as e:\n",
    "            date = 'N/A'\n",
    "\n",
    "        try:\n",
    "            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "        except Exception as e:\n",
    "            description = 'N/A'\n",
    "\n",
    "        # create Dictionary with al review data \n",
    "        data_dict = {\n",
    "            'Name' : name,\n",
    "            'Stars' : stars,\n",
    "            'Title' : title,\n",
    "            'Date' : date,\n",
    "            'Description' : description\n",
    "        }\n",
    "\n",
    "        # Add Dictionary in master empty List\n",
    "        data_dicts.append(data_dict)\n",
    "    \n",
    "    return data_dicts\n",
    "\n",
    "# Grab all HTML\n",
    "html_datas = reviewsHtml(reviews_url)\n",
    "\n",
    "# Empty List to Hold all reviews data\n",
    "reviews = []\n",
    "\n",
    "# Iterate all Html page \n",
    "for html_data in html_datas:\n",
    "    \n",
    "    # Grab review data\n",
    "    review = getReviews(html_data)\n",
    "    \n",
    "    # add review data in reviews empty list\n",
    "    reviews += review\n",
    "\n",
    "# Create a dataframe with reviews Data\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "\n",
    "print(df_reviews)\n",
    "\n",
    "# Save data\n",
    "df_reviews.to_csv('reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13ea31-0270-4480-8b00-707c8e627593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
